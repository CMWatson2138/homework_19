{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does backpropagation work? Be as detailed as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have run your model through forward propagation and determined the \"loss\", the model runs through backpropagation to make adjustments to the weights at each node in order to minimize the loss.  In your model you will set the type of loss you want to minimize and the learning rate you want the adjustments done at.  Then behind the scenes the model will make those tiny adjustments backward on each node based on the learning rate.  Then the model is rerun forward to calculate the new loss.  New backpropagation occurs, again based on the learning rate.  Each round of forward propagation and backward propagation taken together is called an epoch.  The model continues to run epochs for as long as you specify unless you implement early stopping callback. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you remember about PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA squashes data with multiple dimensions into 2d format to make the calculations of your model speed up.  There is some loss of data associated with using PCA You can specify allowable loss or number of components that must be kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a multilayer preceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a neural network consisting of an inout layer an ouput layer and at least one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which approach to improving model stability was the most successful for the example in the reading? Why do you think that was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data resulted in the most successful example in the reading. I think this was due to the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
